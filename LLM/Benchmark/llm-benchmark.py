import ast
import os
import logging
import subprocess
import time
import requests
import sys
import argparse
import yaml
import shlex
from datetime import datetime
from openpyxl import Workbook
from evalscope.perf.main import run_perf_benchmark
from evalscope.perf.arguments import Arguments


def get_logger(level=logging.INFO):
    # åˆ›å»ºloggerå®ä¾‹
    logger = logging.getLogger("LLM-Benchmark")
    logger.setLevel(level)
    logger.propagate = False

    if not logger.handlers:
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(level)
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
        console_handler.setFormatter(formatter)
        # æ·»åŠ å¤„ç†å™¨åˆ°logger
        logger.addHandler(console_handler)

    return logger


logger = get_logger(logging.INFO)


class BenchmarkRunner:
    def __init__(self, config_path):
        self.config = self.load_config(config_path)
        if self.config is None:
            raise ValueError("é…ç½®åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
        self.timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        self.result_file = f"{self.config['result_dir']}/{self.config['result_file_prefix']}-{self.timestamp}.xlsx"

    def load_config(self, file_path):
        """åŠ è½½ YAML é…ç½®æ–‡ä»¶ï¼Œå¹¶è§£æå…ƒç»„å­—ç¬¦ä¸²"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                conf = yaml.safe_load(f)
            # è½¬æ¢å­—ç¬¦ä¸²å…ƒç»„ä¸ºå®é™… tuple
            test_case = conf.get('test_case', {})
            context = [ast.literal_eval(c) for c in test_case.get('context', [])]
            batch_request = [ast.literal_eval(b) for b in test_case.get('batch_request', [])]
            conf['test_case']['context'] = context
            conf['test_case']['batch_request'] = batch_request
            return conf
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        except yaml.YAMLError as e:
            logger.error(f"YAML è§£æé”™è¯¯: {e}")
        except Exception as e:
            logger.error(f"åŠ è½½é…ç½®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

    def generate_test_cases(self):
        """æ ¹æ® mode ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
        mode = self.config['test_case']['mode']
        context = self.config['test_case']['context']
        batch_request = self.config['test_case']['batch_request']
        if mode == 1:
            return [(ctx, br) for ctx in context for br in batch_request]
        elif mode == 2:
            if len(context) != len(batch_request):
                logger.error("mode=2 æ—¶ context ä¸ batch_request é•¿åº¦å¿…é¡»ä¸€è‡´")
                return []
            return list(zip(context, batch_request))
        else:
            logger.error(f"ä¸æ”¯æŒçš„æµ‹è¯•æ¨¡å¼: {mode}")
            return []

    def restart_local_container(self):
        """é‡å¯æœ¬æœº Docker å®¹å™¨"""
        container = self.config['container_name']
        # ä»é…ç½®æ–‡ä»¶è·å–é‡å¯å‘½ä»¤
        restart_cmd_template = self.config.get('restart_cmd', 'docker restart {container_name}')
        cmd = restart_cmd_template.format(container_name=shlex.quote(container))
        logger.info(f"ğŸ”„ é‡å¯æœ¬æœºå®¹å™¨: {container}")
        try:
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            if result.returncode == 0:
                logger.info("âœ… æœ¬æœºå®¹å™¨é‡å¯æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ æœ¬æœºé‡å¯å¤±è´¥: {result.stderr.strip()}")
                return False
        except subprocess.TimeoutExpired:
            logger.error("âŒ æœ¬æœºé‡å¯è¶…æ—¶")
            return False
        except Exception as e:
            logger.error(f"âŒ æœ¬æœºé‡å¯å¼‚å¸¸: {e}")
            return False

    def restart_remote_container(self, host):
        """é€šè¿‡ SSH é‡å¯è¿œç¨‹ä¸»æœºä¸Šçš„å®¹å™¨ï¼ˆéœ€å…å¯†ç™»å½•ï¼‰"""
        ip = host['ip']
        user = host['user']
        port = host.get('port', 22)
        container = self.config['container_name']

        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°åœ°å€
        if ip in ['localhost', '127.0.0.1']:
            return self.restart_local_container()

        # ä»é…ç½®æ–‡ä»¶è·å–é‡å¯å‘½ä»¤
        restart_cmd_template = self.config.get('restart_cmd', 'docker restart {container_name}')
        remote_cmd = restart_cmd_template.format(container_name=shlex.quote(container))
        # ä»é…ç½®æ–‡ä»¶è·å–SSHå‘½ä»¤æ¨¡æ¿
        ssh_cmd_template = self.config.get('ssh_cmd',
                                           'ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -p {port} {user}@{ip} {cmd}')
        # æ„å»ºå®Œæ•´çš„SSHå‘½ä»¤
        ssh_cmd_full = ssh_cmd_template.format(port=port, user=user, ip=ip, cmd=remote_cmd)

        # å°†å‘½ä»¤åˆ†å‰²æˆåˆ—è¡¨å½¢å¼ä¾›subprocessä½¿ç”¨
        ssh_cmd_list = shlex.split(ssh_cmd_full)
        logger.info(f"ğŸ”„ é€šè¿‡ SSH é‡å¯è¿œç¨‹å®¹å™¨ ({container}) {user}@{ip}:{port}")
        try:
            result = subprocess.run(ssh_cmd_list, capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                logger.info("âœ… è¿œç¨‹å®¹å™¨é‡å¯æˆåŠŸ")
                return True
            else:
                logger.error(f"âŒ è¿œç¨‹é‡å¯å¤±è´¥ ({ip}): {result.stderr.strip()}")
                return False
        except subprocess.TimeoutExpired:
            logger.error(f"âŒ è¿œç¨‹é‡å¯è¶…æ—¶ ({ip})")
            return False
        except Exception as e:
            logger.error(f"âŒ è¿œç¨‹é‡å¯å¼‚å¸¸ ({ip}): {e}")
            return False

    def restart_model_service(self):
        """é‡å¯æ‰€æœ‰èŠ‚ç‚¹ï¼ˆæœ¬åœ° + è¿œç¨‹ï¼‰ä¸Šçš„æ¨¡å‹å®¹å™¨"""
        hosts = self.config.get('hosts', [])
        if not hosts:
            logger.warning("æœªé…ç½® hostsï¼Œä»…å°è¯•é‡å¯æœ¬æœºå®¹å™¨")
            return self.restart_local_container()

        success = True
        # é‡å¯æ‰€æœ‰è¿œç¨‹èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ç¬¬ä¸€ä¸ªï¼Œå³ masterï¼‰
        for i, host in enumerate(hosts, 1):
            logger.info(f"ğŸ”„ é‡å¯è¿›åº¦: {i}/{len(hosts)} - æ­£åœ¨é‡å¯ {host['ip']}")
            if not self.restart_remote_container(host):
                success = False

        logger.info("âœ… æ‰€æœ‰èŠ‚ç‚¹é‡å¯å®Œæˆ" if success else "âš ï¸ èŠ‚ç‚¹é‡å¯å·²å®Œæˆï¼Œä½†éƒ¨åˆ†èŠ‚ç‚¹å¤±è´¥")
        return success

    def health_check(self):
        """å¥åº·æ£€æŸ¥ï¼šä»…å¯¹ä¸»èŠ‚ç‚¹ï¼ˆç¬¬ä¸€ä¸ª host æˆ–æœ¬æœºï¼‰å‘èµ·è¯·æ±‚"""
        url = self.config['url']
        logger.info(f"å¯¹ {url} æ‰§è¡Œå¥åº·æ£€æŸ¥...")

        logger.info(f"â³æ¨¡å‹å¯åŠ¨æ—¶é—´è¾ƒé•¿ï¼Œ{self.config['healthcheck']['initial_delay']}såå¼€å§‹ç¬¬ä¸€æ¬¡æ£€æŸ¥")
        time.sleep(self.config['healthcheck']['initial_delay'])
        headers = {"Content-Type": "application/json"}
        data = {
            "model": self.config['model_name'],
            "messages": [{"role": "user", "content": "hello"}],
            "max_tokens": 3,
            "temperature": 0.6,
            "top_p": 0.95,
            "stream": False
        }

        for i in range(self.config['healthcheck']['retry_count']):
            try:
                resp = requests.post(url, json=data, headers=headers, timeout=10)
                if resp.status_code == 200:
                    logger.info("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
                    return True
                else:
                    logger.warning(f"å¥åº·æ£€æŸ¥å¤±è´¥ï¼ˆçŠ¶æ€ç  {resp.status_code}ï¼‰ï¼Œ{self.config['healthcheck']['interval']}s åé‡è¯•")
            except requests.RequestException as e:
                logger.warning(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}ï¼Œ{self.config['healthcheck']['interval']}s åé‡è¯•")
            time.sleep(self.config['healthcheck']['interval'])

        logger.error("âŒ å¥åº·æ£€æŸ¥è¶…æ—¶å¤±è´¥")
        return False

    def run_single_benchmark(self, context, batch_req):
        input_tokens, output_tokens = context
        batch_size, request_count = batch_req
        logger.info(f"â–¶ æµ‹è¯•: in={input_tokens}, out={output_tokens}, concurrency={batch_size}, requests={request_count}")

        bench_args = Arguments(
            parallel=[batch_size],
            number=[request_count],
            model=self.config['model_name'],
            url=self.config['url'],
            tokenizer_path=self.config['tokenizer_path'],
            api='openai',
            dataset=self.config['dataset'],
            min_tokens=output_tokens,
            max_tokens=output_tokens,
            min_prompt_length=input_tokens,
            max_prompt_length=input_tokens,
            debug=False,
            extra_args={'ignore_eos': True}
        )

        try:
            result = run_perf_benchmark(bench_args)
            return self.parse_benchmark_result(result[0])
        except SystemExit as e:
            logger.error(f"åŸºå‡†æµ‹è¯•å¼‚å¸¸é€€å‡º (code={e.code})")
            return None
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return None

    def parse_benchmark_result(self, result):
        comment = ""
        failed = int(result.get("Failed requests", 0))
        if failed > 0:
            comment = f"å¤±è´¥è¯·æ±‚æ•°: {failed}/{result.get('Total requests', 'N/A')}"
        return {
            "ttft": result.get("Average time to first token (s)"),
            "tpot": result.get("Average time per output token (s)"),
            "throughput": result.get("Total token throughput (tok/s)"),
            "duration": result.get("Time taken for tests (s)"),
            "comment": comment
        }

    def create_workbook(self):
        wb = Workbook()
        ws = wb.active
        ws.title = "åŸºå‡†æµ‹è¯•ç»“æœ"
        headers = ["è¾“å…¥Tokenæ•°", "è¾“å‡ºTokenæ•°", "å¹¶å‘æ•°", "è¯·æ±‚æ•°", "TTFT(s)", "TPOT(s)", "åå(tokens/s)", "æŒç»­æ—¶é—´(s)", "å¤‡æ³¨"]
        ws.append(headers)
        return wb, ws

    def save_result(self, ws, wb, context, batch_req, result):
        if result is None:
            logger.warning("ç»“æœä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return
        row = [
            context[0], context[1],
            batch_req[0], batch_req[1],
            result["ttft"], result["tpot"],
            result["throughput"], result["duration"],
            result["comment"]
        ]
        ws.append(row)
        os.makedirs(os.path.dirname(self.result_file), exist_ok=True)
        wb.save(self.result_file)
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜: {self.result_file}")

    def run_benchmarks(self):
        test_cases = self.generate_test_cases()
        if not test_cases:
            logger.error("æœªç”Ÿæˆæœ‰æ•ˆæµ‹è¯•ç”¨ä¾‹")
            return

        logger.info(f"å…± {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        wb, ws = self.create_workbook()

        for i, (context, batch_req) in enumerate(test_cases, 1):
            logger.info(f"_PROGRESS_ {i}/{len(test_cases)}")

            if self.config.get('restart_model', False):
                if not self.restart_model_service() or not self.health_check():
                    logger.error("æœåŠ¡é‡å¯æˆ–å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡å½“å‰æµ‹è¯•")
                    # ä»ä¿å­˜ä¸€æ¡ç©ºç»“æœï¼ˆå¯é€‰ï¼‰
                    self.save_result(ws, wb, context, batch_req, None)
                    continue

            result = self.run_single_benchmark(context, batch_req)
            self.save_result(ws, wb, context, batch_req, result)

        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ– LLM æµ‹è¯•å·¥å…·')
    parser.add_argument('-c', '--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    try:
        runner = BenchmarkRunner(args.config)
        runner.run_benchmarks()
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()